{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d7a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions\n",
    "#from bs4 import BeautifulSoup\n",
    "#import requests\n",
    "#import urllib.request,sys,time\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96877df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the website\n",
    "driver = webdriver.Chrome()\n",
    "website = \"https://www.weforum.org/agenda/climate-change\"\n",
    "driver.get(website)\n",
    "sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f3dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape final page number\n",
    "pagenum = driver.find_element(By.XPATH, '//p[@class=\"chakra-text wef-1u0z722\"]')\n",
    "finalpagenum = pagenum.text.replace('1 / ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55150905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'337'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalpagenum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a28132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape article links from multipaginated website\n",
    "\n",
    "columns = ['Article Name', 'Link', 'Author(s)', 'Date', 'Category']\n",
    "websitedf = pd.DataFrame(columns=columns)\n",
    "\n",
    "for num in range(1, int(finalpagenum)+1):\n",
    "    url = website + '?page=' + str(num)\n",
    "    \n",
    "    # read the website\n",
    "    driver.get(url)\n",
    "    sleep(5)\n",
    "    \n",
    "    # scrape article names\n",
    "    # for first two articles\n",
    "    articles1 = driver.find_elements(By.XPATH, '//p[@class=\"chakra-text wef-1nxztse\"]')\n",
    "    articles1_list = []\n",
    "    for article in range(len(articles1)):\n",
    "        articles1_list.append(articles1[article].text)\n",
    "\n",
    "    # for rest of the articles\n",
    "    articles2 = driver.find_elements(By.XPATH, '//p[@class=\"chakra-text wef-78x0fz\"]')\n",
    "    articles2_list = []\n",
    "    for article in range(len(articles2)):\n",
    "        articles2_list.append(articles2[article].text)\n",
    "\n",
    "    # join the two articles list\n",
    "    article_list = articles1_list + articles2_list\n",
    "    \n",
    "    # scrape article links\n",
    "    # for first two articles\n",
    "    links1 = driver.find_elements(By.XPATH, '//p[@class=\"chakra-text wef-1nxztse\"]/a')\n",
    "    links1_list = []\n",
    "    for link in links1:\n",
    "        links1_list.append(link.get_attribute('href'))\n",
    "\n",
    "    # for rest of articles\n",
    "    links2 = driver.find_elements(By.XPATH, '//p[@class=\"chakra-text wef-78x0fz\"]/a')\n",
    "    links2_list = []\n",
    "    for link in links2:\n",
    "        links2_list.append(link.get_attribute('href'))\n",
    "\n",
    "    links_list = links1_list + links2_list\n",
    "    \n",
    "    # scrape author names\n",
    "    authors = driver.find_elements(By.XPATH, '//p[@class=\"chakra-text wef-kefhaq\"]')\n",
    "    authors_list = []\n",
    "    for author in range(len(authors)):\n",
    "        authors_list.append(authors[author].text)\n",
    "    \n",
    "    # scrape dates\n",
    "    dates = driver.find_elements(By.XPATH, '//p[@class=\"chakra-text wef-1iho44l\"]')\n",
    "\n",
    "    # list of dates\n",
    "    dates_list = []\n",
    "    for date in range(len(dates)):\n",
    "        dates_list.append(dates[date].text)\n",
    "        \n",
    "    # scrape the archive category\n",
    "    category = driver.find_elements(By.XPATH, '//a[@class=\"chakra-badge wef-amhzg0\"]')\n",
    "    category_list = []\n",
    "    for cat in range(len(category)):\n",
    "        category_list.append(category[cat].text)\n",
    "        \n",
    "    # store data in to a dataframe\n",
    "    data = list(zip(article_list, links_list, authors_list, dates_list, category_list))\n",
    "    df = pd.DataFrame(data, columns=['Article Name', 'Link', 'Author(s)', 'Date', 'Category'])\n",
    "    df['Date'] = df['Date'].apply(pd.to_datetime)\n",
    "    \n",
    "    # add the df into the websitedf\n",
    "    websitedf = websitedf.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be7e76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5052 entries, 0 to 5051\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   Article Name  5052 non-null   object        \n",
      " 1   Link          5052 non-null   object        \n",
      " 2   Author(s)     5052 non-null   object        \n",
      " 3   Date          5052 non-null   datetime64[ns]\n",
      " 4   Category      5052 non-null   object        \n",
      "dtypes: datetime64[ns](1), object(4)\n",
      "memory usage: 197.5+ KB\n"
     ]
    }
   ],
   "source": [
    "websitedf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a86489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all the links and scrape article content\n",
    "# then save the content into csv\n",
    "\n",
    "contents = []\n",
    "\n",
    "for link in websitedf['Link']:\n",
    "    driver.get(link)\n",
    "    sleep(5)\n",
    "    \n",
    "    # scrape article content\n",
    "    paragraph = driver.find_elements(By.XPATH, '//div[@class=\"wef-foxfue\"]')\n",
    "    allparagraph = []\n",
    "    for para in range(len(paragraph)):\n",
    "        allparagraph.append(paragraph[para].text)\n",
    "    \n",
    "    contents.append(allparagraph)\n",
    "\n",
    "websitedf['Contents'] = contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b11a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "websitedf.to_excel('WEF Climate Change.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b9dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the file\n",
    "climatechange = pd.read_excel('WEF Climate Change.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "702a9f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wei'En\\AppData\\Local\\Temp/ipykernel_25832/2779335578.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missingcontentdf['Contents'] = missingcontent\n"
     ]
    }
   ],
   "source": [
    "missingcontentdf = climatechange[climatechange['Contents'] == '[]']\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "missingcontent = []\n",
    "for link in missingcontentdf['Link']:\n",
    "    # scrape article content\n",
    "    driver.get(link)\n",
    "    sleep(1)\n",
    "    paragraph = driver.find_elements(By.XPATH, '//div[@class=\"wef-11zt3rm\"] | //div[@class=\"st__content-block st__content-block--text\"] | //div[@class=\"wef-6mvns2\"]')\n",
    "    allparagraph = []\n",
    "    for para in range(len(paragraph)):\n",
    "        allparagraph.append(paragraph[para].text)\n",
    "    \n",
    "    missingcontent.append(allparagraph)\n",
    "\n",
    "missingcontentdf['Contents'] = missingcontent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5b5b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatechange.update(missingcontentdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7882abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatechange.to_excel('WEF Climate Change (updated).xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b13f78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
